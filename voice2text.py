# -*- coding: utf-8 -*-
"""voice2text

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z7TJiX4J95RRQ0v7vvt7ZN560EePvhJY
"""

!apt update 
!apt install ffmpeg

!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116
!pip install git+https://github.com/openai/whisper.git 
!pip install diffusers==0.2.4
!pip install transformers scipy ftfy
!pip install "ipywidgets>=7,<8"

import whisper
import cv2
import time
!pip install streamlit
!pip install pyngrok

import streamlit as st

# Define a function for long-running task
def long_running_task():
    for i in range(10):
        time.sleep(1)
        st.progress(i/10)

# Use st.button() to trigger the long_running_task() function
if st.button('Start recording'):
    with st.spinner('Recording in progress...'):
        long_running_task()
    st.success('Recording complete!')

from pyngrok import ngrok

# public_url = ngrok.connect(port=4040, region='eu')
public_url = ngrok.connect(addr="localhost:4040", region='eu')

# !streamlit run /usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py 

print(public_url)

# from google.colab import auth
# from google.colab import drive
# from oauth2client.client import GoogleCredentials
# from pydrive.auth import GoogleAuth
# from pydrive.drive import GoogleDrive
# from pydrive.files import GoogleDriveFile

# # Authenticate and create the PyDrive client
# auth.authenticate_user()
# gauth = GoogleAuth()
# gauth.credentials = GoogleCredentials.get_application_default()
# drive = GoogleDrive(gauth)

# # Mount Google Drive
# # drive.mount('/content/gdrive')
# from google.colab import drive
# drive.mount('/content/drive', force_remount=True)


# # Navigate to the directory where your Streamlit app is saved
# app_dir = 'My Drive/Colab Notebooks'
# app_name = 'voice2text.py'
# import sys
# sys.path.append('/content/drive/My Drive/Colab Notebooks')
# app_file_list = drive.ListFile({'q': f"'root' in parents and trashed=false and title='{app_name}'"}).GetList()
# # app_file_list = drive.ListFile({'q': f"'root' in parents and trashed=false and title='{app_name}'"},).GetList()

# if len(app_file_list) == 1:
#     app_file = app_file_list[0]
#     app_file.GetContentFile(app_name)
#     print(f'Successfully loaded Streamlit app from {app_dir}/{app_name}.')
# else:
#     print(f'Error: Could not find {app_name} in {app_dir}.')

# from pyngrok import ngrok

# ngrok.set_auth_token('token key')

# public_url = ngrok.connect(port='88', region='eu')

# loading model
model = whisper.load_model('small')

# loading audio file
audio = whisper.load_audio('audio file')
# padding audio to 30 seconds
audio = whisper.pad_or_trim(audio)

# generating spectrogram
mel = whisper.log_mel_spectrogram(audio).to(model.device)

# decoding
options = whisper.DecodingOptions()
result = whisper.decode(model, mel, options)

# ready prompt!
prompt = result.text

# adding tips
prompt += 'Try to draw what you see in the picture'
print(prompt)

# generate the image corresponding to the prompt
img_path = "images/" + prompt + ".jpg"
img = cv2.imread(img_path)
cv2.imshow("Image Prompt", img)
cv2.waitKey(0)
cv2.destroyAllWindows()

# provide positive feedback to the child
print("Great job! You did it!")

import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    'CompVis/stable-diffusion-v1-4',
    revision='fp16',
    torcj_dtype=torch.float16,
    use_auth_token=True
)

pipe = pipe.to("cuda")

with torch.autocast('cuda'):
    image = pipe(prompt)['sample'][0]

import matplotlib.pyplot as plt

plt.imshow(image)
plt.title(prompt)
plt.axis('off')
plt.savefig('result.jpg')
plt.show()